{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btDmoiRCRfMp"
      },
      "source": [
        "# **NaiveBayes Classifier**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a3E1pbkSYSF"
      },
      "source": [
        "## 1. Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GKm6PwhiGxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7954cd3-515c-4c2c-8707-4b0b1e045b4b"
      },
      "source": [
        "# 한국어 전처리 라이브러리 \n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2srhF-lp4JxL"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# POS(Part of Speech) tagger\n",
        "from konlpy import tag "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ-s9c77IVUA"
      },
      "source": [
        "#### 1.2 Train data & test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCBnEQrfoL_C"
      },
      "source": [
        "data = {}\n",
        "# training data. input text 와 정답 label (긍정(1), 부정(0)) 로 구성.\n",
        "\n",
        "data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n",
        "                {'text': \"기대했던 것보단 별로였네요.\"},\n",
        "                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n",
        "                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n",
        "                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n",
        "                {'text': \"연기가 좀 별로였습니다.\"},\n",
        "                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n",
        "                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n",
        "                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n",
        "                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n",
        "                ]\n",
        "# test data\n",
        "data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n",
        "                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n",
        "                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n",
        "                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n",
        "                ]\n",
        "\n",
        "train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
        "test_labels = [1, 0, 1, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpgjbzqr6UR4"
      },
      "source": [
        "### 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKwnFBZqMXm-"
      },
      "source": [
        "#### 2.1 한글 형태소 분석기를 이용해서 주어진 데이터를 tokenize\n",
        "\n",
        "[꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEzeYDmPjNLl"
      },
      "source": [
        "# 형태소 분석기 선언\n",
        "morph_analyzer = tag.Kkma() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tftxirx_7sk7"
      },
      "source": [
        "# tokenization 함수 정의\n",
        "def tokenization(data, morph_analyzer):\n",
        "    '''\n",
        "    (input) data: list of data examples.\n",
        "            morph_analyzer: morphological analyzer.\n",
        "    (output) tokenized_data: list of tokenized data examples.\n",
        "    '''\n",
        "    tokenized_data = []\n",
        "\n",
        "    for example in tqdm(data):\n",
        "        tokens = morph_analyzer.morphs(example['text'])\n",
        "        tokenized_data.append(tokens)\n",
        "\n",
        "    return tokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4VEZyFlCqi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d629bfb5-b69b-4b09-c56e-fd3d2d6b2a88"
      },
      "source": [
        "# tokenization 함수를 이용한 데이터 tokenization\n",
        "tokenized_data = {}\n",
        "\n",
        "tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n",
        "tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 29.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEhn3uv2o2kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d33cbbc-918c-4a99-bebe-cf24126c5b00"
      },
      "source": [
        "# tokenized_data 확인\n",
        "tokenized_data['train']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n",
              " ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n",
              " ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n",
              " ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n",
              " ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n",
              " ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n",
              " ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n",
              " ['기념일',\n",
              "  '에',\n",
              "  '방문',\n",
              "  '하',\n",
              "  '었',\n",
              "  '는데',\n",
              "  '연기',\n",
              "  '도',\n",
              "  '연출',\n",
              "  '도',\n",
              "  '다',\n",
              "  '좋',\n",
              "  '았',\n",
              "  '습니다',\n",
              "  '.'],\n",
              " ['전반적',\n",
              "  '으로',\n",
              "  '지루',\n",
              "  '하',\n",
              "  '었',\n",
              "  '습니다',\n",
              "  '.',\n",
              "  '저',\n",
              "  '는',\n",
              "  '별',\n",
              "  '로',\n",
              "  '이',\n",
              "  '었',\n",
              "  '네요',\n",
              "  '.'],\n",
              " ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olCz4j6VS0KJ"
      },
      "source": [
        "#### 2.2 tokenization 결과를 이용해서 word to index dictionary 를 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2DXYDTTTChQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53249dc4-708c-4b5c-c548-7561d15aa244"
      },
      "source": [
        "# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n",
        "tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i] ]\n",
        "# unique token\n",
        "unique_train_tokens = set(tokens)\n",
        "\n",
        "# NaiveBayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n",
        "word2index = defaultdict() # key: word, value: index of word\n",
        "idx = 0\n",
        "for token in tqdm(unique_train_tokens):\n",
        "    word2index[token] = idx\n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56/56 [00:00<00:00, 482301.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85oCOe0Xqcwk"
      },
      "source": [
        "### 3. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3uuFi52qjh6"
      },
      "source": [
        "#### 3.1 NaiveBayes Classifier 모델 클래스 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsZlgjkBHAod"
      },
      "source": [
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, word2index, k=0.1):\n",
        "        \"\"\"\n",
        "        (input) word2index: mapping a word to a pre-assigned index.\n",
        "        \"\"\"\n",
        "        self.k = k # for smoothing\n",
        "        self.word2index = word2index\n",
        "        self.priors = {} # Prior probability for each class, P(c)\n",
        "        self.likelihoods = {} # Likelihood for each token, P(d|c)\n",
        "\n",
        "    def _set_priors(self, labels):\n",
        "        \"\"\"\n",
        "        Set prior probability for each class, P(c).\n",
        "        Count the number of each class and calculate P(c) for each class.\n",
        "        \"\"\"\n",
        "        \n",
        "        class_counts = defaultdict(int)\n",
        "        ############################ ANSWER HERE ################################\n",
        "        # TODO 1: Count the number of each class\n",
        "        # TODO 2: For each class, calcuate P(c)\n",
        "        #########################################################################        \n",
        "        for c in labels:\n",
        "          class_counts[c]+=1\n",
        "        for c, cnt in class_counts.items():\n",
        "          self.priors[c] = class_counts[c]/len(labels)\n",
        "\n",
        "    def _set_likelihoods(self, tokens, labels):\n",
        "        \"\"\"\n",
        "        Set likelihood for each token, P(d|c).\n",
        "        First, count the number of each class for each token.\n",
        "        Then, calculate P(d|c) for a given class and token.\n",
        "        \"\"\"\n",
        "        # distribute token\n",
        "        token_dists = {}\n",
        "        class_counts = defaultdict(int)\n",
        "\n",
        "        for i, label in enumerate(tqdm(labels)):\n",
        "            count = 0\n",
        "\n",
        "            ############################ ANSWER HERE ################################\n",
        "            # TODO: Count the number of each class for each token.\n",
        "            #########################################################################\n",
        "\n",
        "            # Tokenized Data vs word2Index\n",
        "            for token in tokens[i]:\n",
        "              if token not in token_dists:\n",
        "                token_dists[token] = {0:0, 1:0}\n",
        "              token_dists[token][label]+=1\n",
        "            count += 1\n",
        "            class_counts[label] += count\n",
        "\n",
        "        # Chain Rule?\n",
        "        for token, dist in tqdm(token_dists.items()):\n",
        "            if token not in self.likelihoods:\n",
        "                self.likelihoods[token] = {\n",
        "                    0: (token_dists[token][0]+ self.k) / (class_counts[0] + len(self.word2index)* self.k),\n",
        "                    1: (token_dists[token][1]+ self.k) / (class_counts[1] + len(self.word2index)* self.k),\n",
        "                }\n",
        "\n",
        "    def train(self, input_tokens, labels):\n",
        "        \"\"\"\n",
        "        (input) input_tokens: list of tokenized train data.\n",
        "                labels: train labels for each sentence/document.\n",
        "        \"\"\"\n",
        "        self._set_priors(labels)\n",
        "        self._set_likelihoods(input_tokens, labels)\n",
        "\n",
        "    def inference(self, input_tokens):\n",
        "        \"\"\"\n",
        "        (input) input_tokens: list_of tokenized test data.\n",
        "        \"\"\"\n",
        "        log_prob_0 = 0.0\n",
        "        log_prob_1 = 0.0\n",
        "\n",
        "        # why not production / log -> summation?!\n",
        "        for token in input_tokens:\n",
        "            if token in self.likelihoods:\n",
        "                log_prob_0 += math.log(self.likelihoods[token][0])\n",
        "                log_prob_1 += math.log(self.likelihoods[token][1])\n",
        "\n",
        "        log_prob_0 += math.log(self.priors[0])\n",
        "        log_prob_1 += math.log(self.priors[1])\n",
        "\n",
        "        if log_prob_0 >= log_prob_1:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzjVUyBOQEJk"
      },
      "source": [
        "#### 3.2 주어진 학습 데이터에 대해 문장 분류 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt-iUEVRNsRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277520fa-0e97-4dc7-d1e1-d1ac25795ef0"
      },
      "source": [
        "# 문장 분류 모델 선언 및 학습\n",
        "# word2index { unique Token(word) : index } -> tokenized_data (Whole)\n",
        "classifier = NaiveBayesClassifier(word2index)\n",
        "classifier.train(tokenized_data['train'], train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 55043.36it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 306633.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier.likelihoods\n",
        "result = sorted(result.items(), key=lambda x:x[0], reverse=False)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Ue84IzzJhM",
        "outputId": "f76a76bd-ff61-4a04-e478-0e8872c69459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('!', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('.', {0: 0.5754716981132074, 1: 0.5754716981132074}),\n",
              " ('CG', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('ㄴ', {0: 0.1981132075471698, 1: 0.009433962264150943}),\n",
              " ('ㅂ니다', {0: 0.009433962264150943, 1: 0.29245283018867924}),\n",
              " ('가', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('것', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('겠', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('고', {0: 0.10377358490566037, 1: 0.1981132075471698}),\n",
              " ('기념일', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('기대', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('네요', {0: 0.29245283018867924, 1: 0.009433962264150943}),\n",
              " ('는', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('는데', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('다', {0: 0.009433962264150943, 1: 0.1981132075471698}),\n",
              " ('다는', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('다시', {0: 0.10377358490566037, 1: 0.10377358490566037}),\n",
              " ('더', {0: 0.1981132075471698, 1: 0.009433962264150943}),\n",
              " ('도', {0: 0.009433962264150943, 1: 0.5754716981132074}),\n",
              " ('들', {0: 0.10377358490566037, 1: 0.10377358490566037}),\n",
              " ('로', {0: 0.29245283018867924, 1: 0.009433962264150943}),\n",
              " ('만족', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('방문', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('배우', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('별', {0: 0.29245283018867924, 1: 0.009433962264150943}),\n",
              " ('보', {0: 0.10377358490566037, 1: 0.10377358490566037}),\n",
              " ('보다', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('분', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('생각', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('스럽', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('습니다', {0: 0.29245283018867924, 1: 0.3867924528301886}),\n",
              " ('신경', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('싶', {0: 0.10377358490566037, 1: 0.10377358490566037}),\n",
              " ('쓰', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('안', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('았', {0: 0.009433962264150943, 1: 0.1981132075471698}),\n",
              " ('어서', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('었', {0: 0.5754716981132074, 1: 0.1981132075471698}),\n",
              " ('에', {0: 0.10377358490566037, 1: 0.10377358490566037}),\n",
              " ('연기', {0: 0.10377358490566037, 1: 0.29245283018867924}),\n",
              " ('연출', {0: 0.009433962264150943, 1: 0.29245283018867924}),\n",
              " ('완전', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('으로', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('으면', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('이', {0: 0.3867924528301886, 1: 0.1981132075471698}),\n",
              " ('재미있', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('저', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('전반적', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('정말', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('조금', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('좀', {0: 0.10377358490566037, 1: 0.009433962264150943}),\n",
              " ('좋', {0: 0.10377358490566037, 1: 0.1981132075471698}),\n",
              " ('지루', {0: 0.1981132075471698, 1: 0.009433962264150943}),\n",
              " ('최고', {0: 0.009433962264150943, 1: 0.1981132075471698}),\n",
              " ('추천', {0: 0.009433962264150943, 1: 0.10377358490566037}),\n",
              " ('하', {0: 0.29245283018867924, 1: 0.1981132075471698})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h79XWrsnQJtN"
      },
      "source": [
        "### 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjk05W136d5o"
      },
      "source": [
        "각각의 Test 데이터에 대해 정답 추론 및 Accuracy score 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe-fOScGNzH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f1ccb1-8759-4877-839d-9c4b6fb7b474"
      },
      "source": [
        "# Test 데이터 inference\n",
        "preds = []\n",
        "for test_tokens in tqdm(tokenized_data['test']):\n",
        "    pred = classifier.inference(test_tokens)\n",
        "    preds.append(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 23629.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrYMTKM10vYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c7530b-3e9e-4a95-c595-222ce56249b8"
      },
      "source": [
        "# Accuracy 측정\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(test_labels, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b68-Bh5zArE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}